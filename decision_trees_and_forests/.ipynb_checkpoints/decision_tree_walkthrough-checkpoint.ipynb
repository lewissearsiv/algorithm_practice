{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Tree Based Classifiers\n",
    "#### Lewis Sears\n",
    "\n",
    "By now, it should be clear that a machine learning algorithm to classify data begins with the same problem. Keeping notation consistent, we have a set of classes, $\\chi = \\{C_{1}, \\ldots, C_{k})$, and we want to create a systematic way to classify some instance $\\vec{x} = (x_1,\\ldots, x_n)$ with vector elements that correspond to the $n$ features of our data set. \n",
    "\n",
    "\n",
    "### Decision Tree\n",
    "\n",
    "For tree based classification algorithms, the simplest place to start is a decision tree. You should be able to tell by the picture below why they are called trees! We start with our data point $\\vec{x}$, and from the top, each node represents a \"question\" about $\\vec{x}$ that dictates the next node that we should send $\\vec{x}$ to. at the end of this intense game of \"20 questions\" (maybe not exactly 20), we should be able to systematically decide what class $\\vec{x}$ is in. The bottom nodes of the tree are called **leafs** and they are encoded with probabilities to determine the probability of classifying $\\vec{x}$ based on pre-classified training data taking the same path down the tree. \n",
    "<img src=\"tree_images/DecisionTreePic.png\" alt=\"drawing\" width=\"550\"/>\n",
    "Imagine we have an animal and we want to classify whether that animal is a bird, a mammal, or a fish. We could ask simple questions to get to the bottom of it pretty quickly shown below: \n",
    "<img src=\"tree_images/simpledecisiontree.png\" alt=\"drawing\" width=\"550\"/>\n",
    "\n",
    "Your head should be screaming right now, **\"But Lewis, how do we measure what questions are the most important and construct this???\"** which is a valid question and the meat of the algorithm. Relax, we'll get to that. Before we do, the first part of our code will be how to create a question for specific features in the data set and to partition the data set based on the questions. We calculate how good a question is by its **Gini impurity**. To compute Gini impurity for a set of data with our $k$ classes in $\\chi$, we sum the squared probabilities $p_{i}$, where $p_{i}$ refers to the probability of correctly classifying $A_{i}$ correctly:\n",
    "\n",
    "$$ I_{G}(p) = 1 - \\underset{i \\in \\{1,\\ldots,k\\}}\\sum p_{i}^2.$$\n",
    "Gini impurity values live on the half open interval $[0,1)$ where we are really looking for a value of $0$, which means the question we asked correctly classifies everything! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "class Question:\n",
    "    \"\"\"A Question to label a node. \"\"\"\n",
    "\n",
    "    def __init__(self, feature, value, target):\n",
    "        self.column = feature\n",
    "        self.value = value\n",
    "        self.target = target      \n",
    "\n",
    "    def evaluate(self, data_frame):\n",
    "        '''This function of the class breaks down how well the question separates classes of the target.'''\n",
    "        \n",
    "        #Initial impurity\n",
    "        counts = np.unique(data_frame[self.target], return_counts = True)\n",
    "        target_impurity = 1\n",
    "        for i in range(len(counts[0])):\n",
    "            probability_of_class = counts[1][i] / float(len(data_frame[self.target]))\n",
    "            target_impurity -= probability_of_class**2\n",
    "        self.initial_impurity = target_impurity\n",
    "        \n",
    "        \n",
    "        # Compare the feature value in an example to the\n",
    "        # feature value in this question.\n",
    "        column_vals = data_frame[self.column]\n",
    "        bool_list = []\n",
    "        for row in column_vals: \n",
    "            if type(row) != str: \n",
    "                bool_list.append(row >= self.value)\n",
    "            if type(row) == str:\n",
    "                bool_list.append(row == self.value)\n",
    "        self.bool_list = bool_list\n",
    "        \n",
    "        #Now partition:\n",
    "        true_rows = [i for i, val in enumerate(self.bool_list) if val] \n",
    "        false_rows = [i for i, val in enumerate(self.bool_list) if not val]\n",
    "        self.true_index = true_rows\n",
    "        self.false_index = false_rows\n",
    "        \n",
    "        #Counts the number of each type of example in a dataset.\n",
    "        true_labels = [data_frame[self.target][i] for i in true_rows]\n",
    "        false_labels = [data_frame[self.target][i] for i in false_rows]\n",
    "        self.true_classes = true_labels\n",
    "        self.false_classes = false_labels\n",
    "        \n",
    "        #Now calculate the gini impurity of each new node\n",
    "        true_impurity = 1\n",
    "        for i in range(len(np.unique(true_labels))):\n",
    "            probability_of_class = np.unique(true_labels, return_counts = True)[1][i] / float(len(true_labels))\n",
    "            true_impurity -= probability_of_class**2\n",
    "        false_impurity = 1\n",
    "        for i in range(len(np.unique(false_labels))):\n",
    "            probability_of_class = np.unique(false_labels, return_counts = True)[1][i] / float(len(false_labels))\n",
    "            false_impurity -= probability_of_class**2\n",
    "        self.gini_impurity = np.array([true_impurity, false_impurity])   \n",
    "        \n",
    "        #Finally, we can evaluate the question as a whole:\n",
    "        prob_true = len(true_labels)/(len(true_labels)+len(false_labels))\n",
    "        probs = np.array([prob_true, 1-prob_true])\n",
    "        self.evaluation = self.initial_impurity - np.sum(self.gini_impurity * probs) \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Apple', 'Apple', 'lemon'], ['grape', 'grape'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Simple training set\n",
    "sample_df = pd.DataFrame({'color': ['green', 'yellow', 'red','red','yellow'], \n",
    "                          'size': [3.0,3.0,1.0,1.0,3.0], \n",
    "                          'type': ['Apple','Apple', 'grape','grape','lemon'] })\n",
    "#Now we can ask the question: Is the size greater than 2?\n",
    "question1 = Question('size', 2, 'type')\n",
    "question1.evaluate(sample_df)\n",
    "question1.true_classes, question1.false_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.44444444, 0.        ])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#It looks like this was a good start! Let's check it's gini impurity.\n",
    "#It should have a perfect 0 for the false group\n",
    "question1.gini_impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37333333333333324"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's evaluate the total question:\n",
    "question1.evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['grape', 'grape'], ['Apple', 'Apple', 'lemon'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#What about a categorical question?\n",
    "#Is the color red?\n",
    "question2 = Question('color', 'red', 'type')\n",
    "question2.evaluate(sample_df)\n",
    "question2.true_classes, question2.false_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.44444444])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#It should have a perfect 0 for the true group\n",
    "question2.gini_impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37333333333333324"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's evaluate the total question:\n",
    "question2.evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They're the same!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating questions\n",
    "Now we have a way to evaluate how good a question is at splitting the data given a concrete question to \"ask\". So naturally, we need to abstract a little and find out how to evaluate a set of all possible questions-or at least a good amount of questions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_question(data_frame)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
