{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Tree Based Classifiers\n",
    "#### Lewis Sears\n",
    "\n",
    "By now, it should be clear that a machine learning algorithm to classify data begins with the same problem. Keeping notation consistent, we have a set of classes, $\\chi = \\{C_{1}, \\ldots, C_{k})$, and we want to create a systematic way to classify some instance $\\vec{x} = (x_1,\\ldots, x_n)$ with vector elements that correspond to the $n$ features of our data set. \n",
    "\n",
    "\n",
    "### Decision Tree\n",
    "\n",
    "For tree based classification algorithms, the simplest place to start is a decision tree. You should be able to tell by the picture below why they are called trees! We start with our data point $\\vec{x}$, and from the top, each node represents a \"question\" about $\\vec{x}$ that dictates the next node that we should send $\\vec{x}$ to. at the end of this intense game of \"20 questions\" (maybe not exactly 20), we should be able to systematically decide what class $\\vec{x}$ is in. The bottom nodes of the tree are called **leafs** and they are encoded with probabilities to determine the probability of classifying $\\vec{x}$ based on pre-classified training data taking the same path down the tree. \n",
    "<img src=\"tree_images/DecisionTreePic.png\" alt=\"drawing\" width=\"550\"/>\n",
    "Imagine we have an animal and we want to classify whether that animal is a bird, a mammal, or a fish. We could ask simple questions to get to the bottom of it pretty quickly shown below: \n",
    "<img src=\"tree_images/simpledecisiontree.png\" alt=\"drawing\" width=\"550\"/>\n",
    "\n",
    "Your head should be screaming right now, **\"But Lewis, how do we measure what questions are the most important and construct this???\"** which is a valid question and the meat of the algorithm. Relax, we'll get to that. Before we do, the first part of our code will be how to create a question for specific features in the data set and to partition the data set based on the questions. We calculate how good a question is by its **Gini impurity**. To compute Gini impurity for a set of data with our $k$ classes in $\\chi$, we sum the squared probabilities $p_{i}$, where $p_{i}$ refers to the probability of correctly classifying $A_{i}$ correctly:\n",
    "\n",
    "$$ I_{G}(p) = 1 - \\underset{i \\in \\{1,\\ldots,k\\}}\\sum p_{i}^2.$$\n",
    "Gini impurity values live on the half open interval $[0,1)$ where we are really looking for a value of $0$, which means the question we asked correctly classifies everything! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "class Question:\n",
    "    \"\"\"A Question to label a node. \"\"\"\n",
    "\n",
    "    def __init__(self, feature, value, target):\n",
    "        self.column = feature\n",
    "        self.value = value\n",
    "        self.target = target      \n",
    "\n",
    "    def evaluate(self, data_frame):\n",
    "        '''This function of the class breaks down how well the question separates classes of the target.'''\n",
    "        \n",
    "        #Initial impurity\n",
    "        counts = np.unique(data_frame[self.target], return_counts = True)\n",
    "        target_impurity = 1\n",
    "        for i in range(len(counts[0])):\n",
    "            probability_of_class = counts[1][i] / float(len(data_frame[self.target]))\n",
    "            target_impurity -= probability_of_class**2\n",
    "        self.initial_impurity = target_impurity\n",
    "        \n",
    "        \n",
    "        # Compare the feature value in an example to the\n",
    "        # feature value in this question.\n",
    "        column_vals = data_frame[self.column]\n",
    "        bool_list = []\n",
    "        for row in column_vals: \n",
    "            if type(row) != str: \n",
    "                bool_list.append(row >= self.value)\n",
    "                self.condition = '>='\n",
    "            if type(row) == str:\n",
    "                bool_list.append(row == self.value)\n",
    "                self.condition = '='\n",
    "        self.bool_list = bool_list\n",
    "        \n",
    "        #Now partition:\n",
    "        true_rows = [i for i, val in enumerate(self.bool_list) if val] \n",
    "        false_rows = [i for i, val in enumerate(self.bool_list) if not val]\n",
    "        self.true_data = data_frame.iloc[true_rows].reset_index(drop = True)\n",
    "        self.false_data = data_frame.iloc[false_rows].reset_index(drop = True)\n",
    "        \n",
    "        #Counts the number of each type of example in a dataset.\n",
    "        true_labels = [data_frame[self.target][i] for i in true_rows]\n",
    "        false_labels = [data_frame[self.target][i] for i in false_rows]\n",
    "        self.true_classes = true_labels\n",
    "        self.false_classes = false_labels\n",
    "        \n",
    "        #Now calculate the gini impurity of each new node\n",
    "        true_impurity = 1\n",
    "        for i in range(len(np.unique(true_labels))):\n",
    "            probability_of_class = np.unique(true_labels, return_counts = True)[1][i] / float(len(true_labels))\n",
    "            true_impurity -= probability_of_class**2\n",
    "        false_impurity = 1\n",
    "        for i in range(len(np.unique(false_labels))):\n",
    "            probability_of_class = np.unique(false_labels, return_counts = True)[1][i] / float(len(false_labels))\n",
    "            false_impurity -= probability_of_class**2\n",
    "        self.gini_impurity = np.array([true_impurity, false_impurity])   \n",
    "        \n",
    "        #Finally, we can evaluate the question as a whole:\n",
    "        prob_true = len(true_labels)/(len(true_labels)+len(false_labels))\n",
    "        probs = np.array([prob_true, 1-prob_true])\n",
    "        self.evaluation = self.initial_impurity - np.sum(self.gini_impurity * probs) \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Apple', 'Apple', 'lemon'], ['grape', 'grape'])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Simple training set\n",
    "sample_df = pd.DataFrame({'color': ['green', 'yellow', 'red','red','yellow'], \n",
    "                          'size': [3.0,3.0,1.0,1.0,3.0], \n",
    "                          'type': ['Apple','Apple', 'grape','grape','lemon'] })\n",
    "#Now we can ask the question: Is the size greater than 2?\n",
    "question1 = Question('size', 2, 'type')\n",
    "question1.evaluate(sample_df)\n",
    "question1.true_classes, question1.false_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.44444444, 0.        ])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#It looks like this was a good start! Let's check it's gini impurity.\n",
    "#It should have a perfect 0 for the false group\n",
    "question1.gini_impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37333333333333324"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's evaluate the total question:\n",
    "question1.evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['grape', 'grape'], ['Apple', 'Apple', 'lemon'])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#What about a categorical question?\n",
    "#Is the color red?\n",
    "question2 = Question('color', 'red', 'type')\n",
    "question2.evaluate(sample_df)\n",
    "question2.true_classes, question2.false_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.44444444])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#It should have a perfect 0 for the true group\n",
    "question2.gini_impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37333333333333324"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's evaluate the total question:\n",
    "question2.evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They're the same! Note that the question breaks down into two \"child\" branches of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>color</th>\n",
       "      <th>size</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>red</td>\n",
       "      <td>1.0</td>\n",
       "      <td>grape</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>red</td>\n",
       "      <td>1.0</td>\n",
       "      <td>grape</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  color  size   type\n",
       "0   red   1.0  grape\n",
       "1   red   1.0  grape"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question2.true_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>color</th>\n",
       "      <th>size</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>green</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yellow</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yellow</td>\n",
       "      <td>3.0</td>\n",
       "      <td>lemon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    color  size   type\n",
       "0   green   3.0  Apple\n",
       "1  yellow   3.0  Apple\n",
       "2  yellow   3.0  lemon"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question2.false_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "question3 = Question('size', 10, 'type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating questions\n",
    "Now we have a way to evaluate how good a question is at splitting the data given a concrete question to \"ask\". So naturally, we need to abstract a little and find out how to evaluate a set of all possible questions-or at least a good amount of questions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_question(data_frame, target):\n",
    "    '''Find the best question in a decision tree based on gini impurity'''\n",
    "    #Question(feature, value, target)\n",
    "    #question.evaluate(self, data_frame).evaluation\n",
    "    \n",
    "    columns = list(data_frame.columns)\n",
    "    features = [val for i, val in enumerate(columns) if val != target ]\n",
    "    data = data_frame[features]\n",
    "    \n",
    "    #We iterate through the columns and their values\n",
    "    best_questions_per_feature = []\n",
    "    for feature in list(data.columns):\n",
    "        values = set(data[feature])\n",
    "        what_question = []\n",
    "        gini_imp = []\n",
    "        for value in values:\n",
    "            question = Question(feature, value, target)\n",
    "            question.evaluate(data_frame)\n",
    "            gini_imp.append(question.evaluation)\n",
    "            what_question.append([feature, question.condition, value])\n",
    "        index = [i for i, val in enumerate(gini_imp) if val == max(gini_imp)]\n",
    "        best_questions_per_feature.append([what_question[index[0]], gini_imp[index[0]]])\n",
    "    best_ginis = [x[1] for x in best_questions_per_feature]\n",
    "    best_gini_index = [i for i, val in enumerate(best_ginis) if val == max(best_ginis)]   \n",
    "    return best_questions_per_feature[best_gini_index[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['color', '=', 'red'], 0.37333333333333324]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For our simple example:\n",
    "optimal_question(sample_df, 'type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Leaf:\n",
    "    \"\"\"A Leaf node classifies data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_frame,target):\n",
    "        self.predictions = np.unique(data_frame[target], return_counts = True)\n",
    "class Node:\n",
    "    \"\"\"This holds a reference to the question, and to the two child nodes for each node.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 question,\n",
    "                 true_branch,\n",
    "                 false_branch):\n",
    "        self.question = question\n",
    "        self.true_branch = true_branch\n",
    "        self.false_branch = false_branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tree(data_frame, target):\n",
    "    \"\"\"Builds the decision tree recursively using the optimal question function.\n",
    "    \"\"\"\n",
    "\n",
    "    # Try partitioing the dataset on each of the unique attribute,\n",
    "    # calculate the information gain,\n",
    "    # and return the question that produces the highest gain.\n",
    "    best_question = optimal_question(data_frame, target)[0]\n",
    "    q_value = optimal_question(data_frame, target)[1]\n",
    "    question = Question(best_question[0],best_question[2], target)\n",
    "    question.evaluate(data_frame)\n",
    "    \n",
    "    # Base case: no further info gain\n",
    "    # Since we can ask no further questions,\n",
    "    # we'll return a leaf.\n",
    "    if q_value == 0:\n",
    "        return Leaf(data_frame, target)    \n",
    "    \n",
    "    # If we reach here, we have found a useful feature / value\n",
    "    # to partition on.\n",
    "    true_data = question.true_data\n",
    "    false_data = question.false_data\n",
    "\n",
    "    # Recursively build the true branch.\n",
    "    true_branch = get_tree(true_data, target)\n",
    "\n",
    "    # Recursively build the false branch.\n",
    "    false_branch = get_tree(false_data, target)\n",
    "\n",
    "    # Return a Question node.\n",
    "    # This records the best feature / value to ask at this point,\n",
    "    # as well as the branches to follow\n",
    "    # dependingo on the answer.\n",
    "    return Node(best_question, true_branch, false_branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = get_tree(sample_df, 'type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code I ripped from google developers that shows our decision tree. \n",
    "def show_tree(node, spacing=\"\"):\n",
    "    \"\"\"World's most elegant tree printing function.\"\"\"\n",
    "\n",
    "    # Base case: we've reached a leaf\n",
    "    if isinstance(node, Leaf):\n",
    "        print (spacing + \"Predict\", node.predictions)\n",
    "        return\n",
    "\n",
    "    # Print the question at this node\n",
    "    print (spacing + str(node.question))\n",
    "\n",
    "    # Call this function recursively on the true branch\n",
    "    print (spacing + '--> True:')\n",
    "    print_tree(node.true_branch, spacing + \"  \")\n",
    "\n",
    "    # Call this function recursively on the false branch\n",
    "    print (spacing + '--> False:')\n",
    "    print_tree(node.false_branch, spacing + \"  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['color', '=', 'red']\n",
      "--> True:\n",
      "  Predict (array(['grape'], dtype=object), array([2]))\n",
      "--> False:\n",
      "  ['color', '=', 'yellow']\n",
      "  --> True:\n",
      "    Predict (array(['Apple', 'lemon'], dtype=object), array([1, 1]))\n",
      "  --> False:\n",
      "    Predict (array(['Apple'], dtype=object), array([1]))\n"
     ]
    }
   ],
   "source": [
    "print_tree(tree)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
