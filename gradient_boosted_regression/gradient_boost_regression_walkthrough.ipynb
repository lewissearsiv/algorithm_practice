{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting for Regression\n",
    "#### Lewis Sears\n",
    "\n",
    "Using tree based methods for regression, we now will write an algorithm that predicts continuous data using a method called gradient boosting. Before the algorithm, why don't we just observe how much more powerful the gradient boosted regressor actually is using sci-kit learn's *GradientBoostingRegressor*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#real estate data for testing\n",
    "df = pd.read_csv('car_data.csv')\n",
    "df_cars = df[['Year', 'Selling_Price', 'Kms_Driven','Owner']]\n",
    "target = df['Present_Price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.ensemble as ml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_cars, target, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr = ml.GradientBoostingRegressor(loss = 'ls', learning_rate = 0.75, n_estimators = 100, max_depth = 3)\n",
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbr.fit(X_train, y_train)\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "boosted_scores = gbr.score(X_train, y_train), gbr.score(X_test, y_test)\n",
    "reg_scores = lr.score(X_train, y_train), lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9999661753150955, 0.8595008046607651)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boosted_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8504954757170343, 0.7991031805544824)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, even though it overfit the training data, the boosted tree regression far outperformed standard linear regression. It is a important to note that boosted models primarily reduce bias of our model, so it's common that they overfit on the training data. In contrast, ensemble methods that use bagging tend to reduce variance at the expense of some bias by training parallel models. This is a critical distinction in the two methods.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "class GradientBoostedRegression(object):\n",
    "    '''Our boosted regression algorithm using tree based regression models.'''\n",
    "    \n",
    "    def __init__(self, learning_rate, tree_depth):\n",
    "        '''\n",
    "        Some initial Hyper parameters:\n",
    "        learning_rate: A number between 0 and 1 that scales the added output of a new tree\n",
    "        tree_depth: The number of trees will we stack together \n",
    "        '''\n",
    "        self.learning_rate = learning_rate\n",
    "        self.depth = tree_depth                          \n",
    "\n",
    "    #define the loss function                  \n",
    "    def Loss_Function(target, predicted):\n",
    "        return 0.5*np.sum((target-predicted))**2\n",
    "    #The derivative is really straightforward\n",
    "    def derivative_loss_function(target, predicted):\n",
    "        return -(target - predicted)\n",
    "        \n",
    "    def fit(self, train_data, train_target):\n",
    "        '''Since this is a regression algorithm, the train_target should be a good continuous target.'''\n",
    "        \n",
    "        data = np.array(train_data)\n",
    "        target = np.array(train_target)\n",
    "        \n",
    "        #initialize residuals:\n",
    "        residuals = target - np.mean(target)\n",
    "        \n",
    "        return self"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
