{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "#### Lew Sears\n",
    "\n",
    "Naive Bayes uses the simple and powerful mathematical rule, Bayes Rule, which in ittself has created many branches of mathematical and statistical research. The general principal is stated mathematically as: \n",
    "$$ P(A | B) = \\frac{P( A\\cap B)}{P(B)}. $$\n",
    "The idea, simply, is that we train a data set to calculate the probability distributions of the target variable given the values of the features. Then, we can classify a new data point by finding the probability of whether that data point should be classified for every group of our target choosing the highest probability. Rigorously, for data with $n$ features, where $\\vec{x}$ represents a vector of explicit features, the probability of some data point being in class $A_{i} \\in \\chi$, with $\\chi = \\{A_1,\\ldots,A_k\\}$ being all classes in the target, is calculated as:\n",
    "$$ P(A_{j} | \\vec{x}) = \\frac{P(A_{j})\\cdot P(\\vec{x} | A_{j})}{P(\\vec{x})}$$\n",
    "Disregarding the denominator since it doesn't help differentiate probabilities between separate classes, if $\\vec{x} = (x_1\\ldots x_n)$ using properties of conditional probabilities:\n",
    "$$ P(A_{j},x_1,\\ldots,x_n) = P(x_{1} | x_2,\\ldots,x_n,A_j)\\cdots P(x_{n-2} | x_{n-1}, x_n, A_{j}) \\cdot P(x_{n-1} | x_n, A_{j})\\cdot P(x_n | A_j)\\cdot P(A_{j}).$$\n",
    "If we could calculate this probability for every class $A_{j}$, then we could definitively find the maximum probability of $\\vec{x}$ and easily classify it! Unfortunately, this is extremely computationally heavy and sometimes the relevant conditional probabilities don't even exist. This is where we incorporate the *naive* element of Naive Bayes. We simplify the product elements to be \n",
    "$$P(x_{i} | x_{i+1},\\ldots, x_{n}, A_{j}) \\approx P(x_i | A_{j}).$$\n",
    "This naive assumption has its drawbacks, but in practice it is truly incredible considering how fast a prediction can be made after it is trained. So knowing this, the pros of Naive Bayes are fast real time predictions, it scales well, and works well for highly dimensional data. The main con, of course, is that the naive assumption of conditional probabilities rarely hold in real life. Nonetheless, it is a powerful classifier and the final algorithm to classify a data point $\\vec{x} = (x_{1},\\ldots,x_{n})$ in a set of classes $\\chi = \\{A_{1},\\ldots,A_{k}\\}$ is as follows:\n",
    "$$ \\text{Class }\\left(\\vec{x}\\right) = \\underset{A_{j}\\in \\chi}{\\text{argmax}} \\left(P(A_{j})\\cdot \\underset{x_i\\in\\vec{x}}{\\Pi} P(x_i | A_{j})\\right).$$\n",
    "\n",
    "**A final note:** How you determine the probability distribution of each column can be a robust process in itself. Every feature could (and often does) have its own unique type of distribution, whether it's normally distributed continuous data, count data with a Poisson distribution, etc. Furthermore, these distributions could vary class to class. If you wanted a truly robust Naive Bayes, I believe it would have to be built out with careful consideration to the data. That being said, our algorithm below is going to be simple and should perform decently well given all data. For every column, we implement a standard normal distribution for each feature by class.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier(object):\n",
    "    '''Naive Bayes algorithm for classifying discrete targets. Be sure to one-hot-encode \n",
    "    any categorical variables. The default classification will be '''\n",
    "    \n",
    "    def fit(self, train_data, train_target):\n",
    "        '''Calculates summary statistics for the training data by class.'''\n",
    "        #Make data np arrays\n",
    "        X = np.array(train_data)\n",
    "        y = np.array(train_target)\n",
    "        self.data = X\n",
    "        self.target = y\n",
    "        self.class_counts = np.unique(y, return_counts = True)\n",
    "        \n",
    "        #Dictionary to separate data already converted to numpy arrays\n",
    "        class_dict = dict()\n",
    "        for class_ in self.class_counts[0]:\n",
    "            class_dict[class_] = X[y == class_]\n",
    "            \n",
    "        #Get summary statistics for every class\n",
    "        class_summary = dict()\n",
    "        for class_ in class_dict.keys():\n",
    "            class_summary[class_] = [[np.mean(class_dict[class_][:, i]), \n",
    "                                      np.std(class_dict[class_][:, i])] for i in \n",
    "                                     range(class_dict[class_].shape[1])]\n",
    "            \n",
    "        \n",
    "        self.class_statistics = class_summary \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    #Now take the summary statistics to predict unseen data using gaussian distributions\n",
    "    def predict(self, test_data):\n",
    "        '''Now take the summary statistics from the test data to predict unseen data using \n",
    "        gaussian distributions.'''\n",
    "        \n",
    "        #Function to calculate the gaussian probability \n",
    "        def gaussian_probability(x, mean, std):\n",
    "            z_score = (x-mean)/std\n",
    "            return np.exp(-0.5*(z_score**2)) * (1/(std*np.sqrt(2*np.pi)))\n",
    "        \n",
    "        class_list = []\n",
    "        probability_list = []\n",
    "        for key in self.class_statistics.keys():\n",
    "            class_list.append(key)\n",
    "            statistics = self.class_statistics[key]\n",
    "            \n",
    "            #Add the total probability of the class first\n",
    "            probability = len(self.target[self.target == key])/len(self.target)\n",
    "            for i in range(len(statistics)):\n",
    "                probability_i = gaussian_probability(test_data[i], \n",
    "                                                    self.class_statistics[key][i][0],\n",
    "                                                    self.class_statistics[key][i][1]) \n",
    "                probability *= probability_i\n",
    "            probability_list.append(probability)\n",
    "        \n",
    "        self.predict_proba = [[class_list[i], probability_list[i]] for i in range(len(class_list))]\n",
    "        self.prediction = class_list[[i for i, val in enumerate(probability_list) if \n",
    "                                      val == max(probability_list)][0]]\n",
    "        return self  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A silly example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'weight': [9, 12, 15, 45, 60, 65], \n",
    "                   'height': [1, 1.3, 2, 2.5, 3, 3.5], \n",
    "                   'aggressive': ['no', 'yes', 'yes', 'no', 'no', 'yes']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['cat','cat','cat','dog','dog','dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remember to one-hot-encode columns!!\n",
    "df['aggressive'] = df['aggressive'].map({'yes': 1, 'no': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['cat', 'dog'], dtype='<U3'), array([3, 3]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_model = NaiveBayesClassifier()\n",
    "classification_model.fit(df, target)\n",
    "classification_model.class_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cat': [[12.0, 2.449489742783178],\n",
       "  [1.4333333333333333, 0.41899350299921784],\n",
       "  [0.6666666666666666, 0.4714045207910317]],\n",
       " 'dog': [[56.666666666666664, 8.498365855987974],\n",
       "  [3.0, 0.408248290463863],\n",
       "  [0.3333333333333333, 0.4714045207910317]]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_model.class_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cat'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_model.predict([15, 2.2, 0])\n",
    "classification_model.prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
