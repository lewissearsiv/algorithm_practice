{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "#### Lew Sears\n",
    "\n",
    "Naive Bayes uses the simple and powerful mathematical rule, Bayes Rule, which in ittself has created many branches of mathematical and statistical research. The general principal is stated mathematically as: \n",
    "$$ P(A | B) = \\frac{P( A\\cap B)}{P(B)}. $$\n",
    "The idea, simply, is that we train a data set to calculate the probability distributions of the target variable given the values of the features. Then, we can classify a new data point by finding the probability of whether that data point should be classified for every group of our target choosing the highest probability. Rigorously, for data with $n$ features, where $\\vec{x}$ represents a vector of explicit features, the probability of some data point being in class $A_{i} \\in \\chi$, with $\\chi = \\{A_1,\\ldots,A_k\\}$ being all classes in the target, is calculated as:\n",
    "$$ P(A_{j} | \\vec{x}) = \\frac{P(A_{j})\\cdot P(\\vec{x} | A_{j})}{P(\\vec{x})}$$\n",
    "Disregarding the denominator since it doesn't help differentiate probabilities between separate classes, if $\\vec{x} = (x_1\\ldots x_n)$ using properties of conditional probabilities:\n",
    "$$ P(A_{j},x_1,\\ldots,x_n) = P(x_{1} | x_2,\\ldots,x_n,A_j)\\cdots P(x_{n-2} | x_{n-1}, x_n, A_{j}) \\cdot P(x_{n-1} | x_n, A_{j})\\cdot P(x_n | A_j)\\cdot P(A_{j}).$$\n",
    "If we could calculate this probability for every class $A_{j}$, then we could definitively find the maximum probability of $\\vec{x}$ and easily classify it! Unfortunately, this is extremely computationally heavy and sometimes the relevant conditional probabilities don't even exist. This is where we incorporate the *naive* element of Naive Bayes. We simplify the product elements to be \n",
    "$$P(x_{i} | x_{i+1},\\ldots, x_{n}, A_{j}) \\approx P(x_i | A_{j}).$$\n",
    "This naive assumption has its drawbacks, but in practice it is truly incredible considering how fast a prediction can be made after it is trained. So knowing this, the pros of Naive Bayes are fast real time predictions, it scales well, and works well for highly dimensional data. The main con, of course, is that the naive assumption of conditional probabilities rarely hold in real life. Nonetheless, it is a powerful classifier and the final algorithm to classify a data point $\\vec{x} = (x_{1},\\ldots,x_{n})$ in a set of classes $\\chi = \\{A_{1},\\ldots,A_{k}\\}$ is as follows:\n",
    "$$ \\text{Class }\\left(\\vec{x}\\right) = \\underset{A_{j}\\in \\chi}{\\text{argmax}} \\left(P(A_{j})\\cdot \\underset{x_i\\in\\vec{x}}{\\Pi} P(x_i | A_{j})\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "    '''Naive Bayes algorithm for classifying discrete targets. Be sure to one-hot-encode \n",
    "    any categorical variables. The default classification will be '''\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "    \n",
    "    def fit(self, train_data, train_target):\n",
    "        '''Calculates summary statistics for the training data by class.'''\n",
    "        #Make data np arrays\n",
    "        X = np.array(train_data)\n",
    "        y = np.array(train_target)\n",
    "        self.data = X\n",
    "        self.target = y\n",
    "        self.class_counts = np.unique(y, return_counts = True)\n",
    "        \n",
    "        #Dictionary to separate data already converted to numpy arrays\n",
    "        class_dict = dict()\n",
    "        for class_ in self.class_counts[0]:\n",
    "            class_dict[class_] = X[y == class_]\n",
    "            \n",
    "        #Get summary statistics for every class\n",
    "        class_summary = dict()\n",
    "        for class_ in class_dict.keys():\n",
    "            class_summary[class_] = [[np.mean(class_dict[class_][:, i]), \n",
    "                                      np.std(class_dict[class_][:, i])] for i in \n",
    "                                     range(class_dict[class_].shape[1])]\n",
    "            \n",
    "        \n",
    "        self.class_statistics = class_summary \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    #Now take the summary statistics to predict unseen data using gaussian distributions\n",
    "    def predict(self, test_data):\n",
    "        '''Now take the summary statistics from the test data to predict unseen data using \n",
    "        gaussian distributions.'''\n",
    "        \n",
    "        #Function to calculate the gaussian probability \n",
    "        def gaussian_probability(x, mean, std):\n",
    "            z_score = (x-mean)/std\n",
    "            return np.exp(-0.5*(z_score**2)) * (1/(std*np.sqrt(2*np.pi)))\n",
    "        \n",
    "        class_list = []\n",
    "        probability_list = []\n",
    "        for key in self.class_statistics.keys():\n",
    "            class_list.append(key)\n",
    "            statistics = self.class_statistics[key]\n",
    "            \n",
    "            #Add the total probability of the class first\n",
    "            probability = len(self.target[self.target == key])/len(self.target)\n",
    "            for i in range(len(statistics)):\n",
    "                probability_i = gaussian_probability(test_data[i], \n",
    "                                                    self.class_statistics[key][i][0],\n",
    "                                                    self.class_statistics[key][i][1]) \n",
    "                probability *= probability_i\n",
    "            probability_list.append(probability)\n",
    "        \n",
    "        self.predict_proba = [[class_list[i], probability_list[i]] for i in range(len(class_list))]\n",
    "        self.prediction = class_list[[i for i, val in enumerate(probability_list) if \n",
    "                                      val == max(probability_list)][0]]\n",
    "        return self  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
