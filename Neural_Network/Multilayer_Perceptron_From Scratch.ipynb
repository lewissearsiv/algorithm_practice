{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Neural Network\n",
    "#### Lew Sears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multilayer perceptron is one of the simplest neural networks and is really old technology. A great video series on youtube, [3Blue1Brown](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi), is a great place to start an intuitive understanding of what a neural network is and visualizing the layers involved. Grant Sanderson is an amazing instructor who creates great visuals to understand some really complex subject ranging from machine learning to topology. \n",
    "\n",
    "Our class below creates a neural network that has a single hidden layer. Since it is our first attempt at writing a program from scratch, it will also have an output that is discrete (making this network a natural classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation Function and Optimization Function\n",
    "\n",
    "We will use the sigmoid activation function for our neural network. It is important that we scale the inputs of every new linear combination of weights between 0 and 1. In more modern techniques, engineers prefer RELU and simpler activation functions since the sigmoid can be a bit cumbersome when scaled for big data, but for our purposes it should work just find.\n",
    "\n",
    "Also, we need a workhorse function to optimize and determine the best values for our weights. This is a pretty obvious choice: Stochastic Gradient Descent. Not only is it computationally cheap, but it is easy to understand and has great theoretical and practical success in a wide variety of situations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For a single layer perceptron\n",
    "class SingleLayerPerceptron(object):\n",
    "    \n",
    "    def __init__(self, learning_rate, iterations):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "        \n",
    "    def fit(self, train, target):\n",
    "        self.weights = np.zeros(1+train.shape[1])\n",
    "        self.cost_list = []\n",
    "        \n",
    "        for i in range(self.iterations):\n",
    "            output = self.activate_input(train)\n",
    "            errors = (target - output)\n",
    "            self.weights[1:] += self.learning_rate * np.dot(train.T, errors)\n",
    "            self.weights[0] += self.learning_rate * errors.sum()\n",
    "            cost = (errors**2).sum() / 2.0\n",
    "            self.cost_list.append(cost)\n",
    "        return self\n",
    "    \n",
    "    def net_input(self, X):\n",
    "        return np.dot(X, self.weights[1:]) + self.weights[0]\n",
    "\n",
    "    def activate_input(self, X):\n",
    "        return sigmoid(self.net_input(X))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.where(self.activate_input(X) >= 0.0, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Let's Try a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.SingleLayerPerceptron at 0x7ffe6b6473d0>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = SingleLayerPerceptron(0.1, 100)\n",
    "X = np.random.normal(0 , 1, size = (1000,2))\n",
    "y = np.sum(X, axis =1)\n",
    "def zero_or_1(r):\n",
    "    if r<0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "z = np.array([zero_or_1(a) for a in y])\n",
    "nn.fit(X,z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.12349919e-02,  3.08667306e+01,  3.10283019e+01])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our data was just normally distributed around zero and our target is whether the sum is greater than zero or not, this looks good! The neural network is weighting both entries equally and the intercept is small. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(1)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.predict([0.3,0.1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
