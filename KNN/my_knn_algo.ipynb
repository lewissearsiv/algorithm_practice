{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN: A good place to start for supervised classification\n",
    "#### Lew Sears"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will write a knn classification algorithm from scratch and then check the results compared to the *Scikit Learn* implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mine/yours/everyone's favorite libraries\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go any further, just remind yourself how to properly write a class. It's quite simple but all new programmers have trouble getting started. Our algorithm will be written as a class so we can store information and streamline a work flow like every package we know in SKlearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just recall some class things:\n",
    "class lews_simple_class:\n",
    "    '''This is a nonsense class'''\n",
    "    \n",
    "    def __init__(self, num):\n",
    "        self.num = num\n",
    "        \n",
    "    def multiply(self, a):\n",
    "        return self.num * a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "practice_class = lews_simple_class(5)\n",
    "practice_class.multiply(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps the workhorse function in a knn classification algorithm is a streamlined distance function. So we begin by writing a function that takes an *n*-dimensional point and a data frame with *n* features and calculates the euclidean distance for every row. When we're working with a lot of data points, we want to make every function as streamlined and fast as possible. You're thinking what I'm thinking: we need to make sure we take advantage of NumPy's CPython implementation. If you don't care about the details just remember that NumPy is FAST.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_distance(point,df):\n",
    "    '''Given a point and a pandas dataframe, numpy_distance computes the euclidean distance '''\n",
    "    try: \n",
    "        return np.sqrt(np.sum((np.array(point) - np.array(df))**2, axis = 1))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    #It helps to add some error messages in case something goes wrong\n",
    "    try:\n",
    "        if len(point) != len(df.columns):\n",
    "            return \"Error: The dimensions of your point and DataFrame don't match!\"\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return \"User Error: Please review input critera.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try a quick example with a simple DataFrame and point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.41421356, 2.82842712, 4.24264069])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.DataFrame({'col1': [1,2,3], 'col2': [1,2,3]})\n",
    "point = [0,0]\n",
    "numpy_distance(point, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! You'll notice we have some \"try\" and \"except\" statements. These are actually straightforward; if the function doesn't work you should check the reasons why and give whoever is using the function a heads up about what they can change. This is a good habit and definitely best practice on any team sharing code. Let's see what happens if two inputs dimensions don't match or if you put in some nonsense:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Error: The dimensions of your point and DataFrame don't match!\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.DataFrame({'col1': [1,2,3], 'col2': [1,2,3]})\n",
    "point = [0,0,0]\n",
    "numpy_distance(point, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'User Error: Please review input critera.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy_distance([0,'apple'], 'cat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The KNN Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNClassifier:\n",
    "    \n",
    "    #initialize the hyperparameter k\n",
    "    def __init__(self, k):\n",
    "        try: \n",
    "            if type(k) != int:\n",
    "                return print(\"k-Value Error:\\n-------------\\n k must be a nonzero positive integer\")\n",
    "        except:\n",
    "            pass\n",
    "        try: \n",
    "            if K < 1:\n",
    "                return print(\"k-Value Error:\\n-------------\\n k must be a nonzero positive integer\")\n",
    "        except:\n",
    "            pass\n",
    "        self.k = k\n",
    "        \n",
    "    #Fit the training data.\n",
    "    #You should recall that KNN doesn't actually calculate anything to fit. It just creates a copy of the data.\n",
    "    def fit(self, X_train, y_train):\n",
    "        '''Makes a copy of training data and the target to train knn'''\n",
    "        if len(X_train) != len(y_train):\n",
    "            return print(\"Dimensionality Error: \\n---------------------\\n Training data and training target dimensions don't match.\")\n",
    "        \n",
    "        #Filter out non numeric rows that may occur in the training data\n",
    "        #Careful the output may not be the same size if you have messy data\n",
    "        X_train_filtered = X_train[X_train.applymap(np.isreal).all(1)]\n",
    "        y_train_filtered = [val for i, val in enumerate(list(y_train)) if X_train.applymap(np.isreal).all(1)[i]]\n",
    "        self.train_data = X_train_filtered\n",
    "        self.train_target = y_train_filtered\n",
    "    \n",
    "    #This is the function we trained earlier\n",
    "    def numpy_distance(point,df):\n",
    "        '''Given a point and a pandas dataframe, numpy_distance computes the euclidean distance '''\n",
    "        \n",
    "        #Just some cleaning:\n",
    "        df_clean = df[df.id.apply(lambda x: x.isnumeric())]\n",
    "        \n",
    "        try: \n",
    "            return np.sqrt(np.sum((np.array(point) - np.array(df))**2, axis = 1))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        #It helps to add some error messages in case something goes wrong\n",
    "        try:\n",
    "            if len(point) != len(df.columns):\n",
    "                return \"Error: The dimensions of your point and DataFrame don't match!\"\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        return \"User Error: Please review input critera.\"\n",
    "    \n",
    "    def predict_fast(self, x_test):\n",
    "        '''Classify unseen data using the k-nearest points in the train data'''\n",
    "        \n",
    "        # First, Make a list of distances:\n",
    "        distances = numpy_distance(x_test, self.train_data)\n",
    "        distances_index = distances.argsort()\n",
    "        \n",
    "        \n",
    "        #Now pick the k-closest points:\n",
    "        k_nearest = [val for i, val in enumerate(list(self.train_target)) if i in distances_index[:self.k]]\n",
    "        \n",
    "        #Count the unique values\n",
    "        counts = np.unique(k_nearest, return_counts=True)\n",
    "        \n",
    "        #Find all of the max value classes:\n",
    "        max_values = counts[0][np.where(counts[1] == max(counts[1]))[0]]\n",
    "        return np.random.choice(max_values,1)[0]\n",
    "    \n",
    "    \n",
    "    #After fitting the model, we make predictions on unseen test data\n",
    "    def predict_tie_break(self, x_test):\n",
    "        '''Classify unseen data using the k-nearest points in the train data'''\n",
    "        \n",
    "        # First, Make a list of distances:\n",
    "        distances = numpy_distance(x_test, self.train_data)\n",
    "        distances_index = distances.argsort()\n",
    "        \n",
    "        \n",
    "        #Now pick the k-closest points:\n",
    "        k_nearest = [val for i, val in enumerate(list(self.train_target)) if i in distances_index[:self.k]]\n",
    "        \n",
    "        #Count the unique values\n",
    "        counts = np.unique(k_nearest, return_counts=True)\n",
    "        \n",
    "        #Find all of the max value classes:\n",
    "        max_values = counts[0][np.where(counts[1] == max(counts[1]))[0]]\n",
    "        \n",
    "        if len(max_values) == 1:\n",
    "            return max_values[0]\n",
    "        \n",
    "        #What if we have a tie situation?\n",
    "        #For this situation, we will iteratively remove a neighbor from consideration until there is a unique max\n",
    "        new_k = self.k - 1\n",
    "        while new_k > 0:\n",
    "            #This is all the same code:\n",
    "            k_nearest = [val for i, val in enumerate(list(self.train_target)) if i in distances_index[:self.k]]\n",
    "            counts = np.unique(k_nearest, return_counts=True)\n",
    "            max_values = counts[0][np.where(counts[1] == max(counts[1]))[0]]\n",
    "            if len(max_values) == 1:\n",
    "                return max_values[0] \n",
    "    \n",
    "    #A different tie-breaker\n",
    "    def predict_imbalanced(self, x_test):\n",
    "        '''If you are working with imbalanced data and want to give priority to minority class,\n",
    "        this prediction function always gives any ties to the minority class.'''\n",
    "        \n",
    "        # First, Make a list of distances:\n",
    "        distances = numpy_distance(x_test, self.train_data)\n",
    "        distances_index = distances.argsort()\n",
    "        \n",
    "        \n",
    "        #Now pick the k-closest points:\n",
    "        k_nearest = [val for i, val in enumerate(list(self.train_target)) if i in distances_index[:self.k]]\n",
    "        \n",
    "        #Count the unique values\n",
    "        counts = np.unique(k_nearest, return_counts=True)\n",
    "        \n",
    "        #Find all of the max value classes:\n",
    "        max_values = counts[0][np.where(counts[1] == max(counts[1]))[0]]\n",
    "        \n",
    "        if len(max_values) == 1:\n",
    "            return max_values[0]\n",
    "        \n",
    "        #If we have a tie situation, just pick the smallest class\n",
    "        return max_values[np.array([self.train_target.count(x) for x in max_values]).argmin()]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created 3 functions, all generally the same except for the tie breaker. The choice of how to break ties can become less important when the training data gets massive. It could be argued that for the sake of speed, one could just leave that part out, in which case *predict_fast* is a good choice. A little more robust, *predict_tie_break* runs a loop removing the farthest of the *k* points until it has a unique nearest neighbor.\n",
    "\n",
    "An interesting idea to consider is the case with large class imbalance. It could make sense to just give to tie to the smaller class to balance since we like to skew our classification model in their favor. That's what my *predict_imbalanced* does. With other classification algorithms, there are more nuanced ways to accomplish this but knn is simple and thats why everybody likes it.  \n",
    "\n",
    "Not my quote, but from some PHd guy who has published some papers on this matter:\n",
    "\n",
    "*Developments in learning from imbalanced data have been mainly motivated by numerous real-life applications in which we face the problem of uneven data representation. In such cases the minority class is usually the more important one and hence we require methods to improve its recognition rates. <br />  \n",
    "-Bartosz Krawczyk*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
